{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CatBoost Exercise - Costa Rican Household Poverty Level Prediction\n",
    "This notebook assumes you are already familiar with the basics of catboost and are ready for some experiments and hyperparameter tuning. \n",
    "\n",
    "We will focus here on the Kaggle task \"Costa Rican Household Poverty Level Prediction\". If you haven't gone through the [walkthrough notebook](a-complete-introduction-and-walkthrough.ipynb) yet, this is the time to do so. The output of the notebook is a csv file named `final.csv`, and is expected to be located under the `other_data` folder.\n",
    "\n",
    "This notebook will guide you through the important parts of the exercise, but will leave a lot of room for you to experiment and try different things.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Add all of your imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\programdata\\anaconda3\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: plotly in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (4.7.1)\n",
      "Requirement already satisfied: graphviz in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (0.14)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.16.5)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (3.1.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.12.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.3.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly->catboost) (1.3.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.4.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\programdata\\anaconda3\\lib\\site-packages (7.5.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets) (5.1.2)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets) (7.8.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.0.1)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: jupyter-client in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.3)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (41.4.0)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.4.2)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.15.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.0.10)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets) (1.12.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\programdata\\anaconda3\\lib\\site-packages (from traitlets>=4.3.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.5.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: Send2Trash in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: nbconvert in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (18.1.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (223)\n",
      "Requirement already satisfied: parso>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.15.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (19.2.0)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: testpath in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.2)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "# Set a few plotting defaults\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.rcParams['patch.edgecolor'] = 'k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's recall the different columns we had in our final dataset. This can be useful for defining later which features are categorical and which are not. Remember that there are also aggregation columns (min, max, std, etc.) but they are not mentioned here for simplicity. One can retrieve them easily by simple string matching like so: `<column_name>-<stat>`, for each desired column and aggregation statistic (one of the following: `['min', 'max', 'sum', 'count', 'std', 'range_']`)\n",
    "* Remember that we dropped some of the columns, so you might have to filter column names that do not appear in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = ['Id', 'idhogar', 'Target']\n",
    "\n",
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone', 'rez_esc-missing']\n",
    "\n",
    "ind_ordered = ['rez_esc', 'escolari', 'age']\n",
    "\n",
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'mebaneduc', 'overcrowding']\n",
    "\n",
    "sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n",
    "        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\n",
    "\n",
    "cat_cols = ['elec', 'female_head' ]\n",
    "cat_cols_with_order = ['walls', 'roof', 'floor', 'walls+roof+floor', 'inst', 'tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed dataset, extract train and test splits (remember that the test split doesn't have any labels, you must sumbit a solution to Kaggle in order to get your performance on the test set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 1A39-E879\n",
      "\n",
      " Directory of C:\\Users\\yyahe\\Google Drive\\ML training\\CatBoost part\n",
      "\n",
      "10/06/2020  07:57    <DIR>          .\n",
      "10/06/2020  07:57    <DIR>          ..\n",
      "10/06/2020  07:57    <DIR>          .ipynb_checkpoints\n",
      "15/03/2020  18:32        10,187,726 1.jpg\n",
      "15/03/2020  18:32        11,118,219 2.jpg\n",
      "15/03/2020  18:32        11,017,917 3.jpg\n",
      "15/03/2020  18:32        11,682,306 4.jpg\n",
      "15/03/2020  18:32        11,705,686 5.jpg\n",
      "07/05/2020  10:18         2,313,477 CatBoost vs. Light GBM vs. XGBoost - Towards Data Science.pdf\n",
      "18/05/2020  10:58            22,288 catboost_exercise.docx\n",
      "10/06/2020  07:57            48,739 catboost_solution.ipynb\n",
      "21/05/2020  13:36            18,485 catboosting.ipynb\n",
      "26/05/2020  11:55            42,251 gridcv_results.csv\n",
      "07/05/2020  10:18           642,613 Hyperparameter Tuning - O'Reilly Media.pdf\n",
      "21/05/2020  13:31    <DIR>          input\n",
      "21/05/2020  13:31         1,320,337 input-20200521T103034Z-001.zip\n",
      "21/05/2020  13:40    <DIR>          other_data\n",
      "21/05/2020  13:50         3,165,732 python_tutorial.ipynb\n",
      "              13 File(s)     63,285,776 bytes\n",
      "               5 Dir(s)  254,727,192,576 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('other_data/final.csv')\n",
    "\n",
    "# Labels for training\n",
    "train_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n",
    "test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])\n",
    "\n",
    "# Extract the training data\n",
    "train_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "test_all_data = final[final['Target'].isnull()]\n",
    "submission_base = test_all_data[['Id', 'idhogar']].copy()\n",
    "test_set = test_all_data.drop(columns = ['Id', 'idhogar', 'Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Because we don't have a lot of data in the training set, it will be best to use cross validation for evaluation instead of splitting the training set into a single train-dev split. You also want your different experiments to be comparable. Therefore, you should **create a single set of CV splits of the data which you will use for all of your experiments**. Think about the time vs variance tradeoff when deciding how many splits you want to have (5 is reasonable). **Remember to shuffle and stratify!**\n",
    "\n",
    "* Note: If you don't know yet what is cross validation, why shuffling the data is important, or what it means to stratify the data when splitting it into train-dev-test splits, this is a great time to do so. It's important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "strkfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1_score(labels, predictions):\n",
    "    # Reshape the predictions as needed\n",
    "    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n",
    "    \n",
    "    metric_value = f1_score(labels, predictions, average = 'macro')\n",
    "    \n",
    "    # Return is name, value, is_higher_better\n",
    "    return 'macro_f1', metric_value, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model, features, labels, test_features, strkfold, cat_features_indices=None):\n",
    "    nfolds = strkfold.n_splits\n",
    "    feature_names = list(features.columns)\n",
    "    # Hold all the predictions from each fold\n",
    "    predictions = pd.DataFrame()\n",
    "    importances = np.zeros(features.shape[1])\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    labels = np.array(labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        \n",
    "        # Dataframe for fold predictions\n",
    "        fold_predictions = pd.DataFrame()\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        model.fit(X_train, y_train,\n",
    "                  cat_features=cat_features_indices,\n",
    "                  eval_set=(X_valid, y_valid))\n",
    "        \n",
    "        # Record the validation fold score\n",
    "        y_pred = model.predict(X_valid)\n",
    "        valid_scores.append(f1_score(y_valid, y_pred, average = 'macro'))\n",
    "        \n",
    "        # Make predictions from the fold as probabilities\n",
    "        fold_probabilitites = model.predict_proba(test_features)\n",
    "        \n",
    "        # Record each prediction for each class as a separate column\n",
    "        for j in range(4):\n",
    "            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n",
    "            \n",
    "        # Add needed information for predictions \n",
    "        fold_predictions['idhogar'] = test_ids\n",
    "        fold_predictions['fold'] = (i+1)\n",
    "        \n",
    "        # Add the predictions as new rows to the existing predictions\n",
    "        predictions = predictions.append(fold_predictions)\n",
    "        \n",
    "        # Feature importances\n",
    "        importances += model.feature_importances_ / nfolds   \n",
    "        \n",
    "        # Display fold information\n",
    "        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n",
    "        \n",
    "    # Feature importances dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': importances})\n",
    "\n",
    "    valid_scores = np.array(valid_scores)\n",
    "    display(f'cross validation F1-score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n",
    "    \n",
    "    # Average the predictions over folds\n",
    "    predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "    \n",
    "    # Find the class and associated probability\n",
    "    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "    predictions = predictions.drop(columns = ['fold'])\n",
    "    \n",
    "    # Merge with the base to have one prediction for each individual\n",
    "    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n",
    "        \n",
    "    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "    \n",
    "    # return the submission and feature importances along with validation scores\n",
    "    return submission, feature_importances, valid_scores, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_by_target(predictions):\n",
    "    predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "\n",
    "    # Find the class and associated probability\n",
    "    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "#     predictions = predictions.drop(co?lumns = ['fold'])\n",
    "\n",
    "    # Plot the confidence by each target\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    sns.boxplot(x = 'Target', y = 'confidence', data = predictions);\n",
    "    plt.title('Confidence by Target');\n",
    "\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    sns.violinplot(x = 'Target', y = 'confidence', data = predictions);\n",
    "    plt.title('Confidence by Target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "You will now expriment with various catboost models. You will go through the following steps:\n",
    "1. Training a basic model for an initial baseline and to warm up\n",
    "2. Selecting categorical features\n",
    "3. Using class weights to compensate for imbalanced data\n",
    "4. Performing grid search over some hypreparameter space\n",
    "5. Performing random search over some hyperparameter space\n",
    "6. Optional: performing bayesian hyperparameter optimization using the hyperopt library\n",
    "\n",
    "Note: in some of the experiments you might want to parallelize the training of different models using sklearn njobs argument. In this case, you must run your code in a .py script. If you indeed do this, save the output of each experiment to a csv file, load it here and present the results in a meaningful way (plots etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "Build a catboost model using the default parameters, and don't define any categorical features explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fold 1, Validation Score: 0.39457, Estimators Trained: 234'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 2, Validation Score: 0.40762, Estimators Trained: 387'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 3, Validation Score: 0.40849, Estimators Trained: 267'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 4, Validation Score: 0.44315, Estimators Trained: 429'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 5, Validation Score: 0.37856, Estimators Trained: 485'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cross validation F1-score: 0.40648 with std: 0.02132.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "default_model = CatBoostClassifier(\n",
    "    random_seed=42,\n",
    "#     verbose=200,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "#     class_weights = [0.25, 0.25, 0.25, 0.25],\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "# scorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\n",
    "# cv_score = cross_val_score(default_model, train_set, train_labels, cv=strkfold, scoring = scorer)\n",
    "\n",
    "_, feature_importances, valid_scores, _ = cross_val(default_model, train_set, \n",
    "                                                    train_labels, test_set, strkfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>phones-per-capita</td>\n",
       "      <td>3.827971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>meaneduc</td>\n",
       "      <td>3.554440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>dependency</td>\n",
       "      <td>3.102202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>age-min</td>\n",
       "      <td>2.582773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>age-std</td>\n",
       "      <td>2.423274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>age-sum</td>\n",
       "      <td>2.308501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>age-max</td>\n",
       "      <td>2.124288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>escolari/age-min</td>\n",
       "      <td>2.004123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>escolari-max</td>\n",
       "      <td>1.996252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>walls+roof+floor</td>\n",
       "      <td>1.658207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  importance\n",
       "97   phones-per-capita    3.827971\n",
       "69            meaneduc    3.554440\n",
       "66          dependency    3.102202\n",
       "204            age-min    2.582773\n",
       "207            age-std    2.423274\n",
       "206            age-sum    2.308501\n",
       "205            age-max    2.124288\n",
       "212   escolari/age-min    2.004123\n",
       "200       escolari-max    1.996252\n",
       "94    walls+roof+floor    1.658207"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances.sort_values(by='importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "As you should already know, catboost has some nice support for categorical features. Experiment with this a bit, and tell catboost which of your features are categorical. Also, play with the `one_hot_max_size` hyperparameter. Understand what this hyperparameter is and how it affects the underlying algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of non binary: 109, # of non float: 187\n"
     ]
    }
   ],
   "source": [
    "non_binary_feat = np.where(train_set.apply(lambda x: x.nunique() == 2) == False)[0]\n",
    "non_float_feat = np.where(train_set.dtypes != np.float)[0]\n",
    "\n",
    "print(f'# of non binary: {len(non_binary_feat)}, # of non float: {len(non_float_feat)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_features_indices = np.intersect1d(non_binary_feat, non_float_feat)\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "model_with_cat = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    one_hot_max_size = 2,\n",
    "#     verbose=200,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = [0.25, 0.25, 0.25, 0.25],\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "\n",
    "_, feature_importances, valid_scores, _ = cross_val(model_with_cat, train_set, \n",
    "                                                    train_labels, test_set, strkfold, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_indices = np.intersect1d(non_binary_feat, non_float_feat)\n",
    "# cat_features_indices = non_float_feat\n",
    "\n",
    "model_with_cat = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    one_hot_max_size = 2,\n",
    "#     verbose=200,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = [0.25, 0.25, 0.25, 0.25],\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "\n",
    "_, feature_importances, valid_scores, _ = cross_val(model_with_cat, train_set,\n",
    "                                                    train_labels, test_set, strkfold, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.sort_values(by='importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_features_indices = np.intersect1d(non_binary_feat, non_float_feat)\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "model_with_cat = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    one_hot_max_size=3,\n",
    "#     verbose=200,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = [0.25, 0.25, 0.25, 0.25],\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "_, feature_importances, valid_scores, _ = cross_val(model_with_cat, train_set,\n",
    "                                                    train_labels, test_set, strkfold, cat_features_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weights\n",
    "As you have seen in the dataset walkthrough notebook, our data is very imbalanced (in terms of label frequency). One popular way to deal with imbalanced data is to weight each sample proportionaly to its inverse label frequency. Experiment with the `class_weights` hyperpameter: does the weighting improve the results? try a few different weighting schemes and see how they affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3803735926305015, 2.093661971830986, 1.6815610859728507, 3.347972972972973]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "label_freq = len(train_labels)/counts/4\n",
    "label_freq = np.flip(label_freq).tolist()\n",
    "print(label_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_features_indices = np.intersect1d(non_binary_feat, non_float_feat)\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "model_with_cat = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    one_hot_max_size=2,\n",
    "    verbose=500,\n",
    "#     logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = label_freq,\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "_, feature_importances, valid_scores, _ = cross_val(model_with_cat, train_set,\n",
    "                                                    train_labels, test_set, strkfold, cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_features_indices = np.intersect1d(non_binary_feat, non_float_feat)\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    use_best_model=True,\n",
    "    one_hot_max_size=2,\n",
    "#     verbose=990,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = label_freq,\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "_, feature_importances_with_cat2, valid_scores_with_cat2, predictions_with_cat2 = cross_val(model, train_set, train_labels, \n",
    "                                                                                            test_set, strkfold, cat_features_indices)\n",
    "plot_confidence_by_target(predictions_with_cat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "label_freq = len(train_labels)/counts/4\n",
    "label_freq1 = np.flip(label_freq).tolist()\n",
    "\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    use_best_model=True,\n",
    "    one_hot_max_size=2,\n",
    "#     verbose=990,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = label_freq1,\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "_, feature_importances_with_cat2a, valid_scores_with_cat2a, predictions_with_cat2a = cross_val(model, train_set, train_labels, \n",
    "                                                                                            test_set, strkfold, cat_features_indices)\n",
    "plot_confidence_by_target(predictions_with_cat2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_features_indices = np.intersect1d(non_binary_feat, non_float_feat)\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    use_best_model=True,\n",
    "    one_hot_max_size=4,\n",
    "#     verbose=990,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = label_freq,\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "_, feature_importances_with_cat4, valid_scores_with_cat4, predictions_with_cat4 = cross_val(model, train_set, train_labels, \n",
    "                                                                                            test_set, strkfold, cat_features_indices)\n",
    "plot_confidence_by_target(predictions_with_cat4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_features_indices = np.intersect1d(non_binary_feat, non_float_feat)\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "#     cat_features=cat_features_indices,\n",
    "    random_seed=42,\n",
    "    use_best_model=True,\n",
    "    one_hot_max_size=3,\n",
    "#     verbose=990,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = label_freq,\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "_, feature_importances_with_cat3, valid_scores_with_cat3, predictions_with_cat3 = cross_val(model, train_set, train_labels, \n",
    "                                                                                            test_set, strkfold, cat_features_indices)\n",
    "plot_confidence_by_target(predictions_with_cat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    random_seed=42,\n",
    "    use_best_model=True,\n",
    "    one_hot_max_size=4,\n",
    "#     verbose=990,\n",
    "    logging_level='Silent',\n",
    "    custom_loss='TotalF1:average=Macro',\n",
    "    class_weights = label_freq,\n",
    "    eval_metric='TotalF1:average=Macro'\n",
    ")\n",
    "\n",
    "_, feature_importances_without_cat, valid_scores_without_cat, predictions_without_cat = cross_val(\n",
    "                                                                                            model, train_set, train_labels, test_set, strkfold)\n",
    "plot_confidence_by_target(predictions_without_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance\n",
    "When comparing the performance of different models, it is often desired to measure the statistical significance of the difference between them. Write a function that takes the CV results of two models, and returns the statistical significance (p-value) that one is better than the other.\n",
    "* Hint: use a t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n env37 python=3.7\n",
    "y\n",
    "!conda install -c conda-forge catboost=0.21\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.113234\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0d1514a2950e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel_onehot2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodel_onehot2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TotalF1:use_weights=false'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   3957\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[1;32m   3958\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3959\u001b[0;31m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[1;32m   3960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[1;32m   1711\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m                 \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m                 \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"init_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m             )\n\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "catboost_onehot2_score = []\n",
    "catboost_onehot4_score = []\n",
    "catboost_without_cat_score = []\n",
    "\n",
    "cat_features_indices = non_float_feat\n",
    "\n",
    "params = {\n",
    "    'custom_loss': 'TotalF1:average=Macro',\n",
    "    'class_weights': label_freq,\n",
    "    'eval_metric': 'TotalF1:average=Macro',\n",
    "    'random_seed': 42,\n",
    "    'verbose': 500,\n",
    "#     'logging_level': 'Silent',\n",
    "    'use_best_model': True,\n",
    "    'one_hot_max_size': 2,\n",
    "    'cat_features': cat_features_indices,\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0:1'\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "model_onehot2 = CatBoostClassifier(**params)\n",
    "\n",
    "rnd = np.random.randint(0, 10000)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_set, train_labels, test_size=0.3, random_state=rnd)\n",
    "model_onehot2.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "model_onehot2.best_score_.get('validation').get('TotalF1:use_weights=false')\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onehot2.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "model_onehot2.best_score_.get('validation').get('TotalF1:use_weights=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot2_score = []\n",
    "onehot4_score = []\n",
    "without_cat_score = []\n",
    "\n",
    "params = {\n",
    "    'custom_loss': 'TotalF1:average=Macro',\n",
    "    'class_weights': label_freq,\n",
    "    'eval_metric': 'TotalF1:average=Macro',\n",
    "    'random_seed': 42,\n",
    "    'logging_level': 'Silent',\n",
    "    'use_best_model': True,\n",
    "    'one_hot_max_size': 2,\n",
    "    'cat_features': cat_features_indices\n",
    "}\n",
    "\n",
    "model_onehot2 = CatBoostClassifier(**params)\n",
    "params.update({'one_hot_max_size': 4})\n",
    "model_onehot4 = CatBoostClassifier(**params)\n",
    "params.update({'cat_features': None})\n",
    "model_without_cat = CatBoostClassifier(**params)\n",
    "\n",
    "for i, random_state in enumerate(random.randint(0, 10000, size=100):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_set, train_labels, test_size=0.3, random_state=random_state)\n",
    "    \n",
    "    model_onehot2.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "    onehot2_score.append(model_onehot2.best_score_.get('validation').get('TotalF1:use_weights=false'))\n",
    "    \n",
    "    model_onehot4.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "    onehot4_score.append(model_onehot4.best_score_.get('validation').get('TotalF1:use_weights=false'))\n",
    "    \n",
    "    model_without_cat.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "    without_cat_score.append(model_without_cat.best_score_.get('validation').get('TotalF1:use_weights=false'))\n",
    "    \n",
    "    print(f'Iteration {i}')\n",
    "    print(f'f1-score of onehot=2 / onehot=4 / without-cat models: {onehot2_score[i]:.2f} / {onehot4_score[i]:.2f} / {without_cat_score[i]:.2f}')\n",
    "    print('-'*30)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the total number of data points\n",
    "n = len(train_labels)\n",
    "#Compute the difference between the results\n",
    "diff = [y - x for y, x in zip(valid_scores_with_cat4, valid_scores_with_cat2)]\n",
    "#Comopute the mean of differences\n",
    "d_bar = np.mean(diff)\n",
    "#compute the variance of differences\n",
    "sigma2 = np.var(diff)\n",
    "#compute the number of data points used for training \n",
    "n1 = 4/5*n\n",
    "#compute the number of data points used for testing \n",
    "n2 = 1/5*n\n",
    "#compute the modified variance\n",
    "sigma2_mod = sigma2 * (1/n + n2/n1)\n",
    "#compute the t_static\n",
    "t_static =  d_bar / np.sqrt(sigma2_mod)\n",
    "from scipy.stats import t\n",
    "#Compute p-value and plot the results \n",
    "Pvalue = ((1 - t.cdf(t_static, n-1))*200)\n",
    "Pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to test the improvements of your different models. \n",
    "\n",
    "What does this tell you about the importance of the variance of a model's results (and not only its average performance)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "Hyperparameter tuning is an important part of training ML models. Before continuing, read the following article that gives an overview of the popular hyperparameter tuning methods: [Hyperparameter Tuning - O'Reilly Media](../../Hyperparameter Tuning - O'Reilly Media.pdf) \n",
    "\n",
    "Throughout this part of the execise, keep in mind that you have limited time and resources. Therefore, you must plan your experiments carefully and not just search over a large hyperparameter space using brute force. \n",
    "\n",
    "Document your progress and explain the decisions you made along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "In this section, instead of considering a large grid of hyperparameters, try to think which hyperparameters are the most important, and tune them in an iterative manner. Start with the most important parameters and then move to controlling the less important ones. Try to understand which interval of values is best for them by initially searching in a coarse grained manner and then choose a more fine grained search space depending on the previous results. For example, when tuning for the optimal learning rate, you can at first try different powers of 10 (0.001, 0.01, 0.1) and then, observing that the best results were for 0.01, perform a second search on a smaller interval, such as (0.005, 0.01, 0.05). \n",
    "\n",
    "You might find it useful to visualize the interactions between different hyperparameters. Sklearn's `GridSearchCV` class has the attribute ```cv_results_``` which contains the evaluation results of the models trained with the different parameters. The following function allows to plot some results nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_groups(cv_results, groupby, plotby, plot_train=False, **subplots_params):\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    for k in df.params.values[0].keys():\n",
    "        df[k] = df[['params']].applymap(lambda x: x[k])\n",
    "    fig, axes = plt.subplots(squeeze=False, **subplots_params)\n",
    "    for (key, key_group), ax in zip(df.groupby(groupby), axes.flat):\n",
    "        sorted_group = key_group.sort_values(by=[plotby])\n",
    "        best_plotby = key_group[plotby].values[np.argmax(key_group.mean_test_score.values)]\n",
    "        best_score = key_group.mean_test_score.max()\n",
    "        ax.errorbar(sorted_group[plotby], sorted_group.mean_test_score, yerr=sorted_group.std_test_score)\n",
    "        if plot_train:\n",
    "            ax.errorbar(sorted_group.iterations, sorted_group.mean_train_score, yerr=sorted_group.std_train_score)\n",
    "        ax.set_title(f\"{groupby}={key}, best={best_score:.3f}, {best_plotby} {plotby}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage example:\n",
    "\n",
    "say we performed grid search over the follwing grid (other parameters omitted): \n",
    "\n",
    "```python\n",
    "param_grid = [{'learning_rate': [0.01, 0.005], 'iterations': [600, 900, 1200, 1500]}, {'learning_rate': [0.03, 0.1], 'iterations': [400, 600, 900]}]\n",
    "gridcv = GridSearchCV(..., param_grid=param_grid)\n",
    "gridcv.fit(...)\n",
    "```\n",
    "\n",
    "We can now use our function to plot the performance of different learning rate as a function of the number of iterations (you can also group by multiple hyperparameters by passing a list of hyperparameter names instead of a single one):\n",
    "```python\n",
    "plot_groups(gridcv.cv_results_, groupby='learning_rate', plotby='iterations', nrows=2, ncols=2, sharey=True, figsize=(14,6))\n",
    "```\n",
    "\n",
    "The output we will get will be something like this (the results are fictitious):\n",
    "![alt text](plot_example.png \"plot_example.png\")\n",
    "\n",
    "Remember that for sklearn's multiprocessing to work you need to run the search in a .py script. In this case you can just the script and save the results to a csv file, load them here and present them using the function above. \n",
    "\n",
    "* **Bonus Note**: To find the optimal number of iterations (number of trees), one has two options. The first, which is the most straighforward, is to treat it as a hyperparameter like all others and tune for it. The second makes use of the `staged_predict` function, which allows prediction using only the first `k` iterations. In this case we will first find the maximum number of iterations we want to check, train with this value in all subsequent models, but assess their performance using the best iteration, which can be found using `staged_predict`. While the second method is more efficient, it doesn't come out-of-the-box with sklearn/catboost, so you can stick with the first method. \n",
    "\n",
    "Happy tuning :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "After getting a sense of the hyperparameter space using grid search, try running some random searches and see if you can improve your results. Recall that for any distribution over a sample space with a finite maximum, the maximum of 60 random\n",
    "observations lies within the top 5% of the true maximum, with 95% probability. You might want to define your sampling distribution based on your observations from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Optional: Bayesian hyperparameter optimization using the hyperopt library\n",
    "Consult with your mentor whether you should learn about bayesian hyperparameter optimization or not. If you decide you should, the `hyperopt` library is a popular choice on Kaggle competitions and seems to work well.\n",
    "\n",
    "Definitely add a short `hyperopt` guide/refernce to this exercise if you end up learning it thoroughly and find it valuable!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to Kaggle\n",
    "After you found the best catboost model, you are encouraged to submit it to the Kaggle competition (late submission). Just go to the competition's web page and follow the submission instructions. Remember to train your model on the entire training set when submitting, as you want the highest performance you can get. You can also try using an **ensemble of models** (if you are not familiar with ensembles yet, look it up) trained with different random seeds to improve your score even more :)\n",
    "\n",
    "## The End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
